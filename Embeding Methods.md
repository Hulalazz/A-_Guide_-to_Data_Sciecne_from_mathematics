# Representation Learning

`Representation Learning`, `Manifold Learning` and `Metric Learning` are about how to learn the intrinsic structure of data.
[The objective of representation and metric learning is to build new spaces of representations to improve the performance of a classification, regression or clustering algorithm either from distance constraints or by making use of fine decomposition of instances in complete samples. ](https://laboratoirehubertcurien.univ-st-etienne.fr/en/teams/data-intelligence/recherch-areas/metric-and-representation-learning.html)

[A model’s quality depends completely on the data representation used to train it. For this reason, methods that can transform input data such that it better suits a given machine learning method can improve that method’s predictive capacity.](http://williamlacava.com/research/)

[Unsupervised learning of useful features, or representations, is one of the most basic challenges of machine learning. Too often the success of a data science project depends on the choice of features used. Machine learning has made great progress in training classification, regression and recognition systems when “good” representations, or features, of input data are available. However, much human effort is spent on designing good features which are usually knowledge-based and engineered by domain-experts over years of trial and error. A natural question to ask then is “Can we automate the learning of useful features from raw data?”. Unsupervised representation learning techniques capitalize on unlabeled data which is often cheap and abundant and sometimes virtually unlimited. The goal of these ubiquitous techniques is to learn a representation that reveals intrinsic low-dimensional structure in data, disentangles underlying factors of variation by incorporating universal AI priors such as smoothness and sparsity, and is useful across multiple tasks and domains. This course will focus on theory and methods for representation learning that can easily scale to large amounts of unlabeled, multi-modal, and heterogeneous data.](http://www.cs.jhu.edu/~raman/Courses/CS479f16.html)

Manifold learning and finding low-dimensional structure in data is an important task.

<img src="https://ars.els-cdn.com/content/image/1-s2.0-S2405918816300459-gr1_lrg.jpg" width="70%" />

- [ ] [Representation Learning: A Review and New Perspectives](https://arxiv.org/abs/1206.5538)
- [ ] [Symbolic Representation Learning](http://williamlacava.com/research/symbolic-representation-learning)
- [ ] [Representation Learning on Networks](http://snap.stanford.edu/proj/embeddings-www/)
- [ ] [Representation learning in graph and manifold](https://rlgm.github.io/)
- [ ] [Learning and Reasoning with Graph-Structured Representations, ICML 2019 Workshop](https://graphreason.github.io/)
- [ ] [An Overview on Data Representation Learning: From Traditional Feature Learning to Recent Deep Learning](https://arxiv.org/abs/1611.08331)
- [ ] [4th Workshop on Representation Learning for NLP](https://sites.google.com/view/repl4nlp2019)
- [ ] [Workshop on Representation Learning for Complex ](http://mediamining.univ-lyon2.fr/workshop2019/)
- [ ] [Representation Learning :600.479/679 Fall 2014](http://www.cs.jhu.edu/~raman/Courses/CS679f14.html)
- [ ] [Representation Learning  600.479 Fall 2016](http://www.cs.jhu.edu/~raman/Courses/CS479f16.html)
- [ ] [Representation Learning on Graphs: Methods and Applications](https://www-cs.stanford.edu/people/jure/pubs/graphrepresentation-ieee17.pdf)
- [ ] [DOOCN-XII: Network Representation Learning
Dynamics On and Of Complex Networks 2019](http://doocn.org/)
- [ ] [Representation learning: a unified deep learning framework for automatic prostate MR segmentation](https://www.ncbi.nlm.nih.gov/pubmed/24579148)
- [ ] [Representation Learning Mar. 27 – Mar. 31, 2017@simons.berkeley.edu](https://simons.berkeley.edu/workshops/schedule/3750)
- [ ] [Deep Learning and Representation Learning](https://www.microsoft.com/en-us/research/project/deep-learning-and-representation-learning/)
- [ ] [Robustness beyond Security: Representation Learning](http://gradientscience.org/robust_reps/)
- [ ] [ACL Special Interest Group on Representation Learning](http://www.sigrep.org/)
- [CSC 2414H: Metric Embeddings](http://www.cs.toronto.edu/~avner/teaching/S6-2414/)
- [CS294-158 Deep Unsupervised Learning Spring 2019](https://sites.google.com/view/berkeley-cs294-158-sp19/home)
- [Deep Unsupervised Learning NUS SoC, 2019/2020, Semester I](https://www.comp.nus.edu.sg/~kanmy/courses/6101_1910/)
- https://www.cs.cmu.edu/~anupamg/adv-approx/lecture20.pdf

## Embedding Methods

+ http://paperreading.club/
+ https://ai-distillery.io/
+ https://amds123.github.io/
+ http://jalammar.github.io/
* [Network Embedding](https://shiruipan.github.io/project/effective-network-embedding/)
* [如何评价百度新发布的NLP预训练模型ERNIE? - 知乎](https://www.zhihu.com/question/316140575/answer/719617103)
* [Jure Leskovec.](https://cs.stanford.edu/people/jure/)
* http://www-connex.lip6.fr/~denoyer/wordpress/
* https://blog.feedly.com/learning-context-with-item2vec/
* [Six mathematical gems from the history of Distance Geometry](https://arxiv.org/pdf/1502.02816.pdf)
* [ LOW-DISTORTION EMBEDDINGS OF FINITE
METRIC SPACES](http://www.csun.edu/~ctoth/Handbook/chap8.pdf)
* [On Certain Metric Spaces Arising From Euclidean Spaces by a Change of Metric and Their Imbedding in Hilbert Space](https://www.jstor.org/stable/1968835?seq=1#page_scan_tab_contents)

### Semantic Embedding

<img src="https://carl-allen.github.io/assets/analogy_embeddings.png" width="50%">


+ [semantic matching in information retrieval](http://smir2014.noahlab.com.hk/SMIR2014.htm)
+ [Semantic Web](https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki)
+ [visual semantic embedding](https://github.com/ryankiros/visual-semantic-embedding)
+ [SEMBED: Semantic Embedding of Egocentric Action Videos](https://dimadamen.github.io/SEMBED/ECCVW2016SEMBED.pdf)
+ http://smir2014.noahlab.com.hk/paper%204.pdf
+ https://joeybose.github.io/assets/vse--.pdf
+ https://blog.lateral.io/2017/09/semantic-trees-hierarchical-softmax/
+ [''Analogies Explained'' ... Explained](https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html)
+ [Deep Embedding Logistic Regression](https://www.cse.wustl.edu/~z.cui/papers/DELR_ICBK.pdf)
+ [DeViSE: A Deep Visual-Semantic Embedding Model](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41473.pdf)
+ [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf)
+ [Semantic Embedding for Sketch-Based 3D Shape Retrieval](http://bmvc2018.org/contents/papers/0040.pdf)
+ [A Dual Attention Network with Semantic Embedding for Few-shot Learning](https://xmhe.bitbucket.io/papers/stanet_aaai19.pdf)
+ [Semantic AI: Bringing Machine Learning and Knowledge Graphs Together](https://www.brighttalk.com/webcast/9059/315735/semantic-ai-bringing-machine-learning-and-knowledge-graphs-together)
+ https://2018.eswc-conferences.org/

#### Deep Semantic Embedding

- [Deep Semantic Embedding](http://ceur-ws.org/Vol-1204/papers/paper_4.pdf)
- [Learning Deep Structured Semantic Models for Web Search using Clickthrough Data ](https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf)
- [Siamese LSTM for Semantic Similarity Analysis](https://amitojdeep.github.io/amitoj-blogs/2017/12/31/semantic-similarity.html)
- [Deep Learning Embedding/](https://weimin17.github.io/2017/10/Deep-Learning-Embedding/)
- [Deep Structural Network Embedding (KDD 2016) in Keras Dec. 16th 2017](http://xiaohan2012.github.io/2017/deep-structral-network-embedding-keras/)
- [Deep Learning of Knowledge Graph Embeddings for Semantic Parsing of Twitter Dialogs](https://www.microsoft.com/en-us/research/wp-content/uploads/2014/12/1569992497.pdf)

### Knowledge Graph Embeddings

[Knowledge graphs represent information via entities and their relationships. This form of relational knowledge representation has a long history in logic and artificial intelligence. More recently, it has also been the basis of the Semantic Web to create a “web of data” that is readable by machines.](https://mnick.github.io/project/knowledge-graph-embeddings/)

- [Knowledge Graph Embeddings](https://mnick.github.io/project/knowledge-graph-embeddings/)
- [Knowledge Graph Embedding: A Survey of Approaches and Applications](https://persagen.com/files/misc/Wang2017Knowledge.pdf)
- [Must-read papers on knowledge representation learning (KRL) / knowledge embedding (KE)](https://github.com/thunlp/KRLPapers)
- [Probabilistic Knowledge Graph Embeddings](http://approximateinference.org/2018/accepted/SalehiEtAl2018.pdf)
- [Knowledge Graph Embeddings with node2vec for Item Recommendation](https://2018.eswc-conferences.org/files/posters-demos/paper_265.pdf)

### Word Embedding

- https://www.cntk.ai/pythondocs/index.html
- https://allenai.github.io/allennlp-docs/index.html
- https://lidchallenge.github.io/
- https://wabyking.github.io/talks/
- http://mostafadehghani.com/
- http://ruder.io/word-embeddings-2017/
- http://xiaohan2012.github.io/articles/
- [Simple and efficient semantic embeddings for rare words, n-grams, and language features](http://www.offconvex.org/2018/09/18/alacarte/)
- [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)
- [Arithmetic Properties of Word Embeddings](https://helboukkouri.github.io/blog/arithmetic-properties-of-word-embeddings)
- [Word Embedding Models](https://shorttext.readthedocs.io/en/latest/tutorial_wordembed.html)
- [Embeddings, NN, Deep Learning, Distributional Semantics … in NLP](https://www.cs.upc.edu/~ageno/anlp/embeddings.pdf)

<img src="https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png" width="50%" />

#### word2vec

In natural language processing, the word can be regarded as the node in a graph, which only takes the relation of locality or context.
It is difficult to learn the concepts or the meaning of words. The word embedding technique `word2vec` maps the words to fixed length real vectors:
$$
f: \mathbb{W}\to\mathbb{V}^d\subset \mathbb{R}^d.
$$

The `skip-gram model` assumes that a word can be used to generate the words that surround it in a text sequence.
We assume that, given the central target word, the context words are generated independently of each other.

The conditional probability of generating the context word for the given central target word can be obtained by performing a softmax operation on the vector inner product:
$$
P(w_o|w_c) = \frac{\exp(u_o^T u_c)}{\sum_{i\in\mathbb{V}} \exp(u_i^T u_c)},
$$

where vocabulary index set $V = \{1,2,\dots, |V|-1\}$. Assume that a text sequence of length ${T}$  is given, where the word at time step  ${t}$  is denoted as  $w^{(t)}$.
Assume that context words are independently generated given center words. When context window size is  ${m}$ , the likelihood function of the skip-gram model is the joint probability of generating all the context words given any center word
$$
\prod_{t=1}^{T}\prod_{-m\leq j \leq m, j\not = i}{P}(w^{(t+j)}|w^{(j)}),
$$

Here, any time step that is less than 1 or greater than ${T}$  can be ignored.

The skip-gram model parameters are the central target word vector and context word vector for each individual word. In the training process, we are going to learn the model parameters by maximizing the likelihood function, which is also known as maximum likelihood estimation. his is equivalent to minimizing the following loss function:
$$
-\log(\prod_{t=1}^{T}\prod_{-m\leq j \leq m, j\not = i}{P}(w^{(t+j)}\mid w^{(j)}))
= \\ -\sum_{t=1}^{T}\sum_{-m\leq j \leq m, j \not= i} \log({P}(w^{(t+j)}|w^{(j)}))).
$$

And we could compute the negative logarithm of the conditional probability
$$
-\log(P(w_o|w_c))
= -\log(\frac{\exp(u_o^T u_c)}{\sum_{i\in\mathbb{V}} \exp(u_i^T u_c)})
\\= -u_o^T u_c + \log(\sum_{i\in\mathbb{V}} \exp(u_i^T u_c)).
$$

Then we could compute the gradient or Hessian matrix of the loss functions to update the parameters such as:

$$
\frac{\partial \log(P(w_o|w_c))}{\partial u_c}
    \\= \frac{\partial }{\partial u_c} [u_o^T u_c - \log(\sum_{i\in\mathbb{V}} \exp(u_i^T u_c))]
    \\= u_o - \sum_{j\in\mathbb{V}}\frac{ \exp(u_j^T u_c)) }{\sum_{i\in\mathbb{V}} \exp(u_i^T u_c))} u_j
    \\= u_o - \sum_{j\in\mathbb{V}}P(w_j|w_c) u_j.
$$

The `continuous bag of words (CBOW)` model is similar to the skip-gram model. The biggest difference is that the CBOW model assumes that the central target word is generated based on the context words before and after it in the text sequence. Let central target word  $w_c$  be indexed as $c$ , and context words  $w_{o_1},\cdots, w_{o_{2m}}$  be indexed as  $o_1,\cdots,o_{2m}$  in the dictionary. Thus, the conditional probability of generating a central target word from the given context word is

$$
P(w_c|w_{o_1},\cdots, w_{o_{2m}}) = \frac{\exp(\frac{1}{2m}u_c^T(u_{o_1}+ \cdots + u_{o_{2m}}))}{\sum_{i\in V}\exp(\frac{1}{2m} u_i^T(u_{o_1}+ \cdots + u_{o_{2m}}))}.
$$
`hierarchical softmax, negative sample`
- https://code.google.com/archive/p/word2vec/
- https://skymind.ai/wiki/word2vec
- https://arxiv.org/abs/1402.3722v1
- https://zhuanlan.zhihu.com/p/35500923
- https://zhuanlan.zhihu.com/p/26306795
- https://zhuanlan.zhihu.com/p/56382372
- http://anotherdatum.com/vae-moe.html
- https://d2l.ai/chapter_natural-language-processing/word2vec.html
- https://www.gavagai.io/text-analytics/word-embeddings/
- http://jalammar.github.io/illustrated-word2vec/
- https://devmount.github.io/GermanWordEmbeddings/
- [Contextual String Embeddings for Sequence Labeling](http://alanakbik.github.io/papers/coling2018.pdf)

#### Doc2Vec

- [ ] https://blog.csdn.net/Walker_Hao/article/details/78995591
- [Distributed Representations of Sentences and Documents](http://proceedings.mlr.press/v32/le14.pdf)
- [Sentiment Analysis using Doc2Vec](http://linanqiu.github.io/2015/10/07/word2vec-sentiment/)
- [From Word Embeddings To Document Distances](http://mkusner.github.io/publications/WMD.pdf)



***
- [Learning and Reasoning with Graph-Structured Representations, ICML 2019 Workshop](https://graphreason.github.io/index.html)
- [Statistical Models of Language](http://cocosci.princeton.edu/publications.php?topic=Statistical%20Models%20of%20Language)
- [Semantic Word Embeddings](https://www.offconvex.org/2015/12/12/word-embeddings-1/)
- [Word Embeddings](https://synalp.loria.fr/python4nlp/posts/embeddings/)
- [GloVe: Global Vectors for Word Representation Jeffrey Pennington, Richard Socher,   Christopher D. Manning](https://nlp.stanford.edu/projects/glove/)
- [Word embedding](https://levyomer.wordpress.com/category/word-embeddings/)
- [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing? Friday, November 2, 2018](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)
- [Deep Semantic Embedding](http://smir2014.noahlab.com.hk/paper%204.pdf)
- [无监督词向量/句向量?:W2v/Glove/Swivel/ELMo/BERT](https://x-algo.cn/index.php/2018/11/12/3083/)
- [ ] [The Expressive Power of Word Embeddings](https://arxiv.org/abs/1301.3226)
- [ ] [word vector and semantic similarity](https://spacy.io/usage/vectors-similarity)

#### Transformers, BERT, XLNet and Beyond

Since 2018, pre-training has without a doubt become one of the hottest research topics in Natural Language Processing (NLP). By leveraging generalized language models like the BERT, GPT and XLNet, great breakthroughs have been achieved in natural language understanding. However, in sequence to sequence based language generation tasks, the popular pre-training methods have not achieved significant improvements. Now, researchers from Microsoft Research Asia have introduced MASS—a new pre-training method that achieves better results than BERT and GPT.

<img title="The Encoder-Attention-Decoder framework" src="https://www.microsoft.com/en-us/research/uploads/prod/2019/06/MASS-Fig-1.png" width="60%" />
<img title="MASS framework" src="https://www.microsoft.com/en-us/research/uploads/prod/2019/06/MASS-Fig-4.png" width="60%" />

https://zhuanlan.zhihu.com/p/70257427
https://zhuanlan.zhihu.com/p/51413773
* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
* [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) ](http://jalammar.github.io/illustrated-bert/)
* [BERT-is-All-You-Need](https://github.com/Eurus-Holmes/BERT-is-All-You-Need)
* [Transformer结构及其应用--GPT、BERT、MT-DNN、GPT-2 - 知乎](https://zhuanlan.zhihu.com/p/69290203)
* [放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较](https://zhuanlan.zhihu.com/p/54743941)
* [Universal Transformers](https://mostafadehghani.com/2019/05/05/universal-transformers/)
* [BertEmbedding](https://bert-embedding.readthedocs.io/en/latest/api_reference/bert_embedding.html)
* http://nlp.seas.harvard.edu/2018/04/03/attention.html
* [Visualizing and Measuring the Geometry of BERT](https://arxiv.org/abs/1906.02715)
* [MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450)
* https://pair-code.github.io/interpretability/bert-tree/
* https://www.cs.cmu.edu/~rsalakhu/


### Entity Embedding

* [Entity Embeddings of Categorical Variables](https://arxiv.org/abs/1604.06737)
* https://mc.ai/entity-embedding/
* https://github.com/dkn22/embedder
* https://tidymodels.github.io/embed/index.html

#### Item2Vec

- https://arxiv.org/abs/1603.04259
- https://blog.csdn.net/chris_xy/article/details/78168586

### Gradient Boosted Categorical Embedding and Numerical Trees

`Gradient Boosted Categorical Embedding and Numerical Trees (GB-CSENT)` is to combine Tree-based Models and Matrix-based Embedding Models in order to handle numerical features and large-cardinality categorical features.
A prediction is based on:

* Bias terms from each categorical feature.
* Dot-product of embedding features of two categorical features,e.g., user-side v.s. item-side.
* Per-categorical decision trees based on numerical features ensemble of numerical decision trees where each tree is based on one categorical feature.

In details, it is as following:
$$
\hat{y}(x) = \underbrace{\underbrace{\sum_{i=0}^{k} w_{a_i}}_{bias} + \underbrace{(\sum_{a_i\in U(a)} Q_{a_i})^{T}(\sum_{a_i\in I(a)} Q_{a_i}) }_{factors}}_{CAT-E} + \underbrace{\sum_{i=0}^{k} T_{a_i}(b)}_{CAT-NT}.
$$
And it is decomposed as the following table.
_____
Ingredients| Formulae| Features
---|---|---
Factorization Machines |$\underbrace{\underbrace{\sum_{i=0}^{k} w_{a_i}}_{bias} + \underbrace{(\sum_{a_i\in U(a)} Q_{a_i})^{T}(\sum_{a_i\in I(a)} Q_{a_i}) }_{factors}}_{CAT-E}$ | Categorical Features
GBDT |$\underbrace{\sum_{i=0}^{k} T_{a_i}(b)}_{CAT-NT}$ | Numerical Features
_________

- http://www.hongliangjie.com/talks/GB-CENT_Boston_2017-09-07.pdf
- [Talk: Gradient Boosted Categorical Embedding and Numerical Trees](http://www.hongliangjie.com/talks/GB-CENT_MLIS_2017-06-06.pdf)
- [Paper: Gradient Boosted Categorical Embedding and Numerical Trees](https://qzhao2018.github.io/zhao/publication/zhao2017www.pdf)
- https://qzhao2018.github.io/zhao/

### Hyperbolic Embeddings

The motivation of hyperbolic embedding is to combine structural information with continuous representations suitable for machine learning methods. One example is embedding taxonomies (such as Wikipedia categories, lexical databases like WordNet, and phylogenetic relations).

The big goal when embedding a space into another is to **preserve distances and more complex relationships**. It turns out that hyperbolic space can better embed graphs (particularly hierarchical graphs like trees) than is possible in Euclidean space. Even better—angles in the hyperbolic world are the same as in Euclidean space, suggesting that hyperbolic embeddings are useful for downstream applications (and not just a quirky theoretical idea).

The motivation is to embed structured, discrete objects such as knowledge graphs into a continuous representation that can be used with modern machine learning methods. Hyperbolic embeddings can preserve graph distances and complex relationships in very few dimensions, particularly for hierarchical graphs.

**Hyperbolic Axiom**: There is a line
$l$ and point $P$ not on $l$, such that there are at least 2 different lines through $P$
parallel to $l$.

This leads to the general property that given a line, there are an `infinite number` of different lines parallel to it through an outside point.
Parallel lines can be of different types. A pair of parallel lines are said to be limiting parallel if
they approach each-other asymptotically `without intersecting`.
Such a pair does not admit a common line perpendicular to both, and the distance between the two does not have a minimum.
A pair of parallel lines are called divergent parallel
if they move away from each-other in both directions.

> Let $X$ be a geodesic space and $\triangle\subset X$ a geodesic triangle; that is, a set of three
geodesic segments in $X$ such that any pair of segments shares precisely one endpoint. Then $\triangle$ is $\delta$-slim if any side of $\triangle$  is contained in the $\delta$-neighbourhood
of the other two. The metric space X is $\delta$-hyperbolic if every triangle is $\delta$-slim,
and $X$ is called hyperbolic if it is $\delta$-hyperbolic for some $\delta\geq 0$.


---
- [Characterizing the analogy between hyperbolic embedding and community structure of complex networks ](http://doocn.org/2018/files/radicchi-slides.pdf)
- [Hyperbolic Embedding search result @Arxiv-sanity](http://www.arxiv-sanity.com/search?q=Hyperbolic+Embeddings)
- [Geometric Representation Learning](https://mnick.github.io/project/geometric-representation-learning/)

<img title="Embedding of the WordNet noun hierarchy into a 2-dimensional hyperbolic space" src="https://mnick.github.io/img/wn-nouns.jpg" width="70%" />

#### Spherical and Hyperbolic Embeddings of Data

We have a set of distances between points, $d_{ij}$. If these points exist on the surface of a hypersphere, then the position vectors of the points in a Euclidean embedding space for the hypersphere should be
$$\left<x_i, x_j\right>=r^2\cos(\frac{d_{ij}}{r}).$$

From this we construct an inner product matrix $Z$ with $Z_{ij}=\left<x_i, x_j\right>$.

This matrix should be positive semidefinite with a smallest eigenvalue of zero. We can use this fact to find the appropriate radius $r$ of the embedding space by minimizing the magnitude of the smallest eigenvalue of $Z$.

Point positions can then be found easily using `standard kernel embedding techniques`. This embedding is exact if the points lie on a hypersphere. Of course real data rarely lies precisely on a spherical manifold and so the paper also discusses details of the inexact case.

This can naturally be extended to hyperbolic embeddings where and we need to optimize the second smallest eigenvalue
$$
\left<x_i, x_j\right>= -r^2\cosh(\frac{d_{ij}}{r})\\
r^{\ast} = \arg\min_{r}|\lambda_2[Z(r)]|.
$$

Here $\lambda_2[Z(r)]$ is the second smallest eigenvalues of  the matrix $Z(r)$, $|\cdot |$ is the absolute value function.

|Stanford bunny texture mapped with spherical texture using spherical embedding|
|:---:|
|<img src="https://www.cs.york.ac.uk/cvpr/embedding/bunny_elliptic.gif" width="40%" />|

- [Spherical and Hyperbolic Embeddings of Data](https://www.cs.york.ac.uk/cvpr/embedding/index.html)
- http://simbad-fp7.eu/
- https://www.cs.york.ac.uk/cvpr/embedding/TPAMI2316836.pdf


#### Embedding of Trees in Hyperbolic Plane

##### Delaunay Embedding of Trees

**Delaunay Graph**: Given a set of vertices in the hyperbolic plane $\mathbb H$ their `Delaunay
graph` is one where a pair of vertices are neighbors if their Voronoi cells intersect.

Here the `Voronoi cell` of $v_i\in\{v_0, v_1,\dots, v_n \}\subset \mathbb H$ is defined as the set of points whose distance to $v_i$ is not larger than the distance to $v_j$ for $j\not=i$, denoted as $\mathcal V(v_i)$.

<img src="http://www.ae.metu.edu.tr/tuncer/ae546/prj/delaunay/dt.gif" width="50%" />

Suppose $T = (V, E, w)$ is a weighted tree,
where $w : E \to \mathbb R$ is the weight or length function on the edges. The goal is to
realize the weight $w(v_i v_j )$ on each edge $v_i v_j$ as the length ${|\phi_i \phi_j |}_{\mathbb H}$ of the edge in the Delaunay embedding of the tree.

- http://www.ae.metu.edu.tr/tuncer/ae546/prj/delaunay/
- [Low Distortion Delaunay Embedding of Trees in Hyperbolic Plane](https://homepages.inf.ed.ac.uk/rsarkar/papers/HyperbolicDelaunayFull.pdf)

|A simple Pythagorean embedding into the vertices of a unit cube|
|:---:|
|<img src="https://pair-code.github.io/interpretability/bert-tree/tree-cube.png" width="80%" />|


- [A randomized embedding algorithm for trees](https://theory.stanford.edu/~jvondrak/data/tree-embedding.pdf)
- [Tree Preserving Embedding](http://www.icml-2011.org/papers/421_icmlpaper.pdf)
- [Language, trees, and geometry in neural networks](https://pair-code.github.io/interpretability/bert-tree/)

#### Poincaré Embeddings


|An attempt at embedding a tree with branching factor 4 into the Euclidean plane | Embedding a hierarchical tree with branching factor two into a 2D hyperbolic plane |
:---|---:
<img src="http://bjlkeng.github.io/images/euclidean_graph_embedding.png" width="45%"/>|<img src="http://bjlkeng.github.io/images/poincare_graph_embedding.png" width="45%"/>

- [Poincaré Embeddings for Learning Hierarchical Representations](https://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hierarchical-representations.pdf)
- [Implementing Poincaré Embeddings](https://rare-technologies.com/implementing-poincare-embeddings/)
- [Hyperbolic Geometry and Poincaré Embeddings](http://bjlkeng.github.io/posts/hyperbolic-geometry-and-poincare-embeddings/)

#### Embedding Networks in Hyperbolic Spaces

Since hyperbolic space is tree-like, it’s natural to consider embedding trees—which we can do with arbitrarily low distortion, for any number of dimensions!
It is shown that [how to extend this technique to arbitrary graphs, a problem with a lot of history in Euclidean space](https://arxiv.org/pdf/1804.03329.pdf) using a two-step strategy for embedding graphs into hyperbolic space:

1. Embed a graph $G = (V, E)$ into a tree $T$.
2. Embed $T$ into the Poincaré ball.

<img src="https://dawn.cs.stanford.edu/assets/img/2018-03-19-hyperbolics/poincare3.png" width="60%" />


- [Hyperbolic Embeddings with a Hopefully Right Amount of Hyperbole](https://dawn.cs.stanford.edu/2018/03/19/hyperbolics/)
- [Hyperbolic Function Embedding: Learning Hierarchical Representation for Functions of Source Code in Hyperbolic Spaces](https://www.mdpi.com/2073-8994/11/2/254/htm)
- [Embedding Networks in Hyperbolic Spaces ](http://bactra.org/notebooks/hyperbolic-networks.html)
- [Statistics on Manifolds](http://bactra.org/notebooks/statistics-on-manifolds.html)
- [Analysis of Network Data](http://bactra.org/notebooks/network-data-analysis.html)
- [Efficient embedding of complex networks to hyperbolic space via their Laplacian](https://www.nature.com/articles/srep30108)
- [Hyperbolic geometry and real life networks](http://www.ec2017.org/celinska/)
- [Neural Embeddings of Graphs in Hyperbolic Space](https://arxiv.org/pdf/1705.10359.pdf)
- [Metric Embedding, Hyperbolic Space, and Social Networks∗](https://sites.cs.ucsb.edu/~suri/psdir/SoCG14.pdf)

#### HyperE

[In this website](https://hazyresearch.github.io/hyperE/), we release hyperbolic embeddings that can be further integrated into applications related to knowledge base completion or can be supplied as features into various NLP tasks such as Question Answering.

---|---
---|---
<img title="combinatorial_tree1" src="https://hazyresearch.github.io/hyperE/pytorch_tree.gif" width="90%" />|<img title = "tree2" src="https://hazyresearch.github.io/hyperE/combinatorial_tree.gif" width="90%" />

- [HyperE: Hyperbolic Embeddings for Entities](https://hazyresearch.github.io/hyperE/)
- [Embedding Text in Hyperbolic Spaces](https://ai.google/research/pubs/pub47117)
+ [Language, trees, and geometry in neural networks](https://pair-code.github.io/interpretability/bert-tree/)

*****

- https://zhuanlan.zhihu.com/p/47489505
- http://blog.lcyown.cn/2018/04/30/graphencoding/
- https://blog.csdn.net/NockinOnHeavensDoor/article/details/80661180
- http://building-babylon.net/2018/04/10/graph-embeddings-in-hyperbolic-space/
- https://paperswithcode.com/task/graph-embedding
- [Representation Tradeoffs for Hyperbolic Embeddings](https://arxiv.org/pdf/1804.03329.pdf)

### Graph Embedding

Graph embedding, preprocessing of graph data processing, is an example of representation learning to find proper numerical representation form of graph data structure.
It maps the graph structure to numerical domain:  $f:\mathbf{G}\mapsto V\subset \mathbb{R}^{n}$.
The goal when embedding a graph $G$ into a space  $V$ is to preserve the graph distance (the shortest path between a pair of vertices) in the space  $V$.

- [Graph Embedding：深度学习推荐系统的"基本操作"](https://zhuanlan.zhihu.com/p/68247149)
- https://github.com/thunlp/NRLpapers
- https://github.com/thunlp/GNNPapers
- http://snap.stanford.edu/proj/embeddings-www/
- https://arxiv.org/abs/1709.05584
- http://cazabetremy.fr/Teaching/EmbeddingClass.html
- https://posenhuang.github.io/
- [Awesome Graph Embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding)
- [A Beginner's Guide to Graph Analytics and Deep Learning](https://skymind.ai/wiki/graph-analysis)
- [15TH INTERNATIONAL WORKSHOP ON
MINING AND LEARNING WITH GRAPHS](http://www.mlgworkshop.org/2019/)
- [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec](https://arxiv.org/abs/1710.02971)
- [Spatially Embedded Networks](http://www2.eng.ox.ac.uk/sen/)
- [DOOCN-XII: Network Representation Learning](http://doocn.org/)
- [Representation Learning on Graphs: Methods and Applications](http://sites.computer.org/debull/A17sept/p52.pdf)

#### DeepWalk

`DeepWalk` is an approach for learning latent representations of vertices in a network, which maps the nodes in the graph into real vectors:
$$
f: \mathbb{V}\to\mathbb{R}^{d}.
$$

[DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences.](http://www.perozzi.net/projects/deepwalk/)
And if we consider the  text as digraph, `word2vec` is an specific example of `DeepWalk`.
Given the word sequence $\mathbb{W}=(w_0, w_1, \dots, w_n)$, we can compute the conditional probability $P(w_n|w_0, w_1, \dots, w_{n-1})$. And
$$P(w_n|f(w_0), f(w_1), \cdots, f(w_{n_1}))$$

> * DeepWalk  $G, w, d, \gamma, t$
>> * Input: graph $G(V, E)$;
     window size $w$;
     embedding size $d$;
     walks per vertex $\gamma$;
     walk length $t$.
>> * Output:  matrix of vertex representations $\Phi\in\mathbb{R}^{|V|\times d}$
>
>> *  Initialization: Sample $\Phi$ from $\mathbb{U}^{|V|\times d}$;
>>     + Build a binary Tree T from V;
>>     + for $i = 0$ to $\gamma$ do
>>        -  $O = Shuffle(V )$
>>        -  for each $v_i \in O$ do
>>        -  $W_{v_i}== RandomWalk(G, v_i, t)$
>>        -  $SkipGram(Φ, W_{v_i}, w)$
>>        - end for
>>     + end for
***

> $SkipGram(Φ, W_{v_i}, w)$
* 1. for each $v_j \in W_{v_i}$ do
    + 2. for each $u_k \in W_{v_i}[j - w : j + w]$ do
    + 3.  $J(\Phi)=-\log Pr(u_k\mid \Phi(v_j))$
    + 4.  $\Phi =\Phi -\alpha\frac{\partial J}{\partial \Phi}$
    + 5. end for
* 6. end for

Computing the partition function (normalization factor) is expensive, so instead we will factorize the
conditional probability using `Hierarchical softmax`.
If the path to vertex $u_k$ is identified by a sequence of tree nodes $(b_0, b_1, \cdots , b_{[log |V|]})$,
and $(b_0 = root, b_{[log |V|]} = u_k)$ then
$$
Pr(u_k\mid \Phi(v_j)) =\prod_{l=1}^{[log |V|]}Pr(b_l\mid \Phi(v_j)).
$$


Now, $Pr(b_l\mid \Phi(v_j))$ could be modeled by a binary classifier
that is assigned to the parent of the node $b_l$ as below
$$
Pr(b_l\mid \Phi(v_j))=\frac{1}{1 + \exp(-\Phi(v_j)\cdot \Phi(b_l))}
$$
where $\Phi(b_l)$ is the representation assigned to tree node
$b_l$ ’s parents.


- [ ] [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652)
- [ ] [DeepWalk at github](https://github.com/phanein/deepwalk)
- [ ] [Deep Walk Project @perozzi.net](http://www.perozzi.net/projects/deepwalk/)
- [ ] http://www.cnblogs.com/lavi/p/4323691.html
- [ ] https://www.ijcai.org/Proceedings/16/Papers/547.pdf

#### node2vec

`node2vec` is an algorithmic framework for representational learning on graphs. Given any graph, it can learn continuous feature representations for the nodes, which can then be used for various downstream machine learning tasks.

By extending the Skip-gram architecture to networks, it seeks to optimize the following objective function,
which maximizes the log-probability of observing a network neighborhood $N_{S}(u)$ for a node $u$ conditioned on its feature representation, given by $f$:
$$
\max_{f} \sum_{u\in V}\log Pr(N_S(u)\mid f(u))
$$

`Conditional independence` and `Symmetry` in feature space are expected  to  make the optimization problem tractable.
We model the conditional likelihood of every source-neighborhood node pair as a softmax
unit parametrized by a dot product of their features:
$$
Pr(n_i\mid f(u))=\frac{\exp(f(n_i)\cdot f(u))}{\sum_{v\in V} \exp(f(v)\cdot f(u))}.
$$

The objective function simplifies to
$$\max_{f}\sum_{u\in V}[-\log Z_u + \sum_{n_i\in N_S(u)} f(n_i)\cdot f(u).$$

- [ ] https://zhuanlan.zhihu.com/p/30599602
- [ ] http://snap.stanford.edu/node2vec/
- [ ] https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf
- [ ] https://www.kdd.org/kdd2016/papers/files/rfp0218-groverA.pdf


#### struc2vec

This is a paper about identifying nodes in graphs that play a similar role based solely on the structure of the graph, for example computing the structural identity of individuals in social networks. That’s nice and all that, but what I personally find most interesting about the paper is the meta-process by which the authors go about learning the latent distributed vectors that capture the thing they’re interested in (structural similarity in this case). Once you’ve got those vectors, you can do vector arithmetic, distance calculations, use them in classifiers and other higher-level learning tasks and so on. As word2vec places semantically similar words close together in space, so we want structurally similar nodes to be close together in space.

Struc2vec has four main steps:

> 1. Determine the structural similarity between each vertex pair in the graph, for different neighborhood sizes.
> 2. Construct a weighted multi-layer graph, in which each layer corresponds to a level in a hierarchy measuring structural similarity (think: ‘at this level of zoom, these things look kind of similar?).
> 3. Use the multi-layer graph to generate context for each node based on biased random walking.
> 4. Apply standard techniques to learn a latent representation from the context given by the sequence of nodes in the random walks.

![](https://adriancolyer.files.wordpress.com/2017/09/struc2vec-sketch-8.jpeg?w=200&zoom=0.1)

- [ ] [STRUC2VEC（图结构→向量）论文方法解读](http://jackieanxis.coding.me/2018/01/17/STRUC2VEC/)
- [ ] [struc2vec: Learning Node Representations from Structural Identity](http://www.land.ufrj.br/~leo/struc2vec.html)
- [ ] https://arxiv.org/abs/1704.03165
- [Struc2vec: learning node representations from structural identity](https://blog.acolyer.org/2017/09/15/struc2vec-learning-node-representations-from-structural-identity/)


#### Gaussian Auto Embeddings

http://koaning.io/gaussian-auto-embeddings.html

#### Atom2Vec

M. V. Diudea, I. Gutman and L. Jantschi wrote in the preface of the book _Molecular Topology_:
[One of the principal goals of chemistry is to establish (causal) relations between the chemical and
physical (experimentally observable and measurable) properties of substance and the
structure of the corresponding molecules. Countless results along these lines have been
obtained, and their presentation comprise significant parts of textbooks of organic,
inorganic and physical chemistry, not to mention treatises on theoretical chemistry](http://www.moleculartopology.com/)

- [ ] [Deep Learning For Molecules and Materials](http://www.rqrm.ca/DATA/TEXTEDOC/03a-total-september2018-v1.pdf)

#### tile2Vec

- [ ] https://ermongroup.github.io/blog/tile2vec/
- [ ] https://arxiv.org/abs/1805.02855

* [RDF2Vec: RDF Graph Embeddings and Their Applications](http://www.semantic-web-journal.net/system/files/swj1495.pdf)
* [EmbedS: Scalable and Semantic-Aware Knowledge Graph Embeddings](https://expolab.org/papers/embeds-slides.pdf)

#### graph2vec

`graph2vec` is to learn data-driven distributed representations of arbitrary sized graphs in an unsupervised manner and are task agnostic.

- [ ] [graph2vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005)
- [ ] https://allentran.github.io/graph2vec
- [ ] http://humanativaspa.it/tag/graph2vec/
- [ ] https://zhuanlan.zhihu.com/p/33732033
- [ ] [Awesome graph embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding)
- [ ] [Graph Embedding Methods](https://github.com/palash1992/GEM)
- [ ] [Graph Embedding @ deep learning pattern](https://www.deeplearningpatterns.com/doku.php?id=graph_embedding)
- [ ] [LINE: Large-scale Information Network Embedding](https://arxiv.org/abs/1503.03578)
- [ ] [Latent Network Summarization: Bridging Network Embedding and Summarization](http://ryanrossi.com/pubs/Jin-et-al-latent-network-summarization.pdf)
- [ ] [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf)
- [ ] [GEMS Lab](http://web.eecs.umich.edu/~dkoutra/group/index.html)
- [Graph Embeddings — The Summary](http://sungsoo.github.io/2019/05/26/graph-embedding.html)
- [Graph Embeddings search result @ Arxiv-sanity](http://www.arxiv-sanity.com/search?q=Graph+Embedding)
- [Semantic Annotation for Places in LBSN through Graph Embedding](https://yangzhangalmo.github.io/papers/CIKM17.pdf)

##  Hierarchical Representation Learning

- [Hierarchical Representations with Poincaré Variational Auto-Encoders](https://deepmind.com/research/publications/hierarchical-representations-poincare-variational-auto-encoders)
- [Nonparametric Variational Auto-encoders for Hierarchical Representation Learnings](https://arxiv.org/abs/1703.07027)
- [Poincaré Embeddings for Learning Hierarchical Representations](https://mnick.github.io/talk/cern2018/)

<img src="https://ask.qcloudimg.com/http-save/yehe-1908973/ih2p67xh4x.jpeg?imageView2/2/w/1620" />

### HARP

- https://so-link.org/seminar/2018-06/harp.pdf
- https://github.com/GTmac/HARP
- [HARP: Hierarchical Representation Learning for Networks](https://arxiv.org/abs/1706.07845)
- http://www.mlgworkshop.org/2017/paper/MLG2017_paper_10.pdf
- https://gtmac.github.io/publication/harp/
